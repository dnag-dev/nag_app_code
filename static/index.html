<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Nag - The Digital Twin</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <style>
    body {
      margin: 0;
      font-family: 'Segoe UI', sans-serif;
      background: linear-gradient(to bottom right, #000000, #1a1a1a);
      color: #fff;
      display: flex;
      flex-direction: column;
      align-items: center;
      justify-content: center;
      height: 100vh;
      overflow: hidden;
    }
    h1 { font-size: 2rem; margin-bottom: 0.2rem; }
    p { margin-top: 0; font-style: italic; opacity: 0.8; }
    #orb {
      width: 120px; height: 120px; border-radius: 50%;
      margin: 2rem auto;
      background: radial-gradient(circle, #00f2ff, #001f3f);
      box-shadow: 0 0 25px rgba(0,255,255,0.6);
      transition: all 0.4s ease-in-out;
      animation: pulseIdle 2s infinite;
    }
    #orb.listening {
      background: radial-gradient(circle, #ff7300, #a62c00);
      animation: pulseListening 1s infinite;
    }
    #orb.speaking {
      background: radial-gradient(circle, #00ff8c, #004d33);
      animation: pulseSpeaking 1s infinite;
    }
    #orb.thinking {
      background: radial-gradient(circle, #ffcc00, #cc7700);
      animation: pulseThinking 1.5s infinite;
    }
    @keyframes pulseIdle {
      0%, 100% { transform: scale(1); box-shadow: 0 0 20px rgba(0,255,255,0.2); }
      50% { transform: scale(1.05); box-shadow: 0 0 30px rgba(0,255,255,0.4); }
    }
    @keyframes pulseListening {
      0%, 100% { transform: scale(1); box-shadow: 0 0 30px rgba(255,115,0,0.3); }
      50% { transform: scale(1.1); box-shadow: 0 0 40px rgba(255,115,0,0.5); }
    }
    @keyframes pulseSpeaking {
      0%, 100% { transform: scale(1); box-shadow: 0 0 30px rgba(0,255,140,0.4); }
      50% { transform: scale(1.1); box-shadow: 0 0 40px rgba(0,255,140,0.6); }
    }
    @keyframes pulseThinking {
      0%, 100% { transform: scale(1); box-shadow: 0 0 30px rgba(255,204,0,0.3); }
      50% { transform: scale(1.08); box-shadow: 0 0 40px rgba(255,204,0,0.5); }
    }
    #debug {
      width: 90%;
      max-height: 250px;
      overflow-y: auto;
      background: #111;
      border: 1px solid #333;
      padding: 10px;
      font-family: monospace;
      font-size: 0.85rem;
      color: #0f0;
      margin-top: 2rem;
    }
    #toggleButton {
      padding: 10px 20px;
      font-size: 1rem;
      margin-top: 1rem;
      border: none;
      background-color: #00f2ff;
      color: #000;
      border-radius: 10px;
      cursor: pointer;
    }
    #toggleButton:disabled {
      background-color: #555;
      cursor: not-allowed;
    }
    .play-button {
      padding: 8px 16px;
      margin: 10px auto;
      display: block;
      background-color: #00ff8c;
      color: #000;
      border: none;
      border-radius: 8px;
      cursor: pointer;
      font-weight: bold;
    }
  </style>
</head>
<body>
  <h1>Welcome to Nag</h1>
  <p>The digital extension of Dinakara's mind ‚Äî therapist, companion, unfiltered mirror.</p>
  <div id="orb" class="idle"></div>
  <audio id="audio" hidden></audio>
  <button id="toggleButton">Start Conversation</button>
  <div id="debug"><strong>Debug Log:</strong></div>

  <script>
    const orb = document.getElementById("orb");
    const audio = document.getElementById("audio");
    const debugBox = document.getElementById("debug");
    const toggleButton = document.getElementById("toggleButton");

    let mediaRecorder;
    let audioChunks = [];
    let stream;
    let listening = false;
    let interrupted = false;
    let currentPlayButton = null;
    let emptyTranscriptionCount = 0;
    
    // Detect iOS
    const isiOS = /iPad|iPhone|iPod/.test(navigator.userAgent) || 
        (navigator.platform === 'MacIntel' && navigator.maxTouchPoints > 1);
    const isSafari = /^((?!chrome|android).)*safari/i.test(navigator.userAgent);
    
    // Flag to track if audio has been unlocked
    let audioUnlocked = false;

    // Function to unlock audio
    function unlockAudio() {
      if (audioUnlocked) return;
      
      // Create and play a silent audio file
      const silentAudio = new Audio("data:audio/mp3;base64,SUQzBAAAAAAAI1RTU0UAAAAPAAADTGF2ZjU4LjI5LjEwMAAAAAAAAAAAAAAA//tQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAWGluZwAAAA8AAAACAAADQgD///////////////////////////////////////////8AAAA8TEFNRTMuMTAwAQAAAAAAAAAAABSAJAJAQgAAgAAAA0L2YLwAAAAAAAAAAAAAAAAAAAAA//sQZAAP8AAAaQAAAAgAAA0gAAABAAABpAAAACAAADSAAAAETEFNRTMuMTAwVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV//sQZB4P8AAAaQAAAAgAAA0gAAABAAABpAAAACAAADSAAAAEVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVU=");
      
      // Try to play it - this may be rejected by Safari
      silentAudio.play().then(() => {
        audioUnlocked = true;
        logDebug("üîä Audio unlocked successfully");
      }).catch(e => {
        logDebug("‚ö†Ô∏è Could not unlock audio automatically: " + e.message);
      });
    }

    // Remove play button if exists
    function removePlayButton() {
      if (currentPlayButton) {
        currentPlayButton.remove();
        currentPlayButton = null;
      }
    }

    // Check if MediaRecorder is supported
    if (!window.MediaRecorder) {
      logDebug("‚ö†Ô∏è Your browser doesn't support MediaRecorder API!");
      toggleButton.disabled = true;
      toggleButton.textContent = "Not supported in this browser";
    }

    function logDebug(msg) {
      const p = document.createElement("p");
      p.textContent = msg;
      debugBox.appendChild(p);
      debugBox.scrollTop = debugBox.scrollHeight;
    }

    async function startListening() {
      try {
        // Clean up any existing play button
        removePlayButton();
        
        // Reset empty transcription counter
        emptyTranscriptionCount = 0;
        
        orb.classList.remove("idle", "speaking", "thinking");
        orb.classList.add("listening");
        logDebug("üéôÔ∏è Listening...");

        stream = await navigator.mediaDevices.getUserMedia({ audio: true });

        // Try different MIME types in order of preference
        let mimeType = "";
        // Prioritize MP4 on iOS/Safari
        const supportedTypes = (isiOS || isSafari) 
          ? ["audio/mp4", "audio/mpeg", "audio/webm", "audio/ogg;codecs=opus", ""]
          : ["audio/webm", "audio/mp4", "audio/mpeg", "audio/ogg;codecs=opus", ""];
        
        for (const type of supportedTypes) {
          if (MediaRecorder.isTypeSupported(type)) {
            mimeType = type;
            logDebug(`Using audio format: ${mimeType || "browser default"}`);
            break;
          }
        }
        
        // Create MediaRecorder with the supported type
        mediaRecorder = new MediaRecorder(stream, mimeType ? { mimeType } : {});
        audioChunks = [];

        mediaRecorder.ondataavailable = e => audioChunks.push(e.data);

        mediaRecorder.onstop = async () => {
          if (interrupted) return;
          orb.classList.remove("listening");
          
          // Use the same MIME type for the blob
          const blob = new Blob(audioChunks, mimeType ? { type: mimeType } : {});
          const formData = new FormData();
          
          // Determine appropriate file extension based on MIME type
          let fileExt = "audio";
          if (mimeType.includes("webm")) fileExt = "webm";
          else if (mimeType.includes("mp4") || mimeType.includes("mpeg")) fileExt = "mp3";
          else if (mimeType.includes("ogg")) fileExt = "ogg";
          
          formData.append("file", blob, `input.${fileExt}`);

          try {
            logDebug("üì§ Uploading voice...");
            const res = await fetch("/transcribe", { 
              method: "POST", 
              body: formData 
            });

            let data;
            try {
              data = await res.json();
            } catch (jsonErr) {
              logDebug("‚ùå JSON parse failed: " + jsonErr.message);
              orb.classList.add("idle");
              // Continue listening after a brief delay
              setTimeout(() => {
                if (!interrupted) startListening();
              }, 1000);
              return;
            }

            const message = (data.transcription || "").trim();
            logDebug("üìù Transcribed: " + (message || "No speech detected"));

            if (!message || message === "undefined") {
              logDebug("‚ö†Ô∏è Empty message. Continuing to listen...");
              emptyTranscriptionCount++;
              
              // If we get too many empty transcriptions in a row, maybe suggest something to the user
              if (emptyTranscriptionCount >= 3) {
                emptyTranscriptionCount = 0; // Reset counter
                await sendToChat("I didn't hear anything. How can I help you today?");
              } else {
                orb.classList.add("idle");
                // Continue listening after a brief delay
                setTimeout(() => {
                  if (!interrupted) startListening();
                }, 1000);
              }
              return;
            }

            // Reset counter on successful transcription
            emptyTranscriptionCount = 0;
            await sendToChat(message);
          } catch (e) {
            logDebug("‚ùå Transcription error: " + e.message);
            orb.classList.add("idle");
            // Continue listening after a brief delay
            setTimeout(() => {
              if (!interrupted) startListening();
            }, 1000);
          }
        };

        mediaRecorder.start();
        
        // Use voice activity detection for better recording
        setupVoiceActivityDetection(stream);
        
        // Set a maximum recording time as a fallback
        setTimeout(() => {
          if (mediaRecorder && mediaRecorder.state === "recording") {
            mediaRecorder.stop();
          }
        }, 7000); // 7 seconds max
      } catch (e) {
        logDebug("üö´ Mic access failed: " + e.message);
        orb.classList.remove("listening");
        orb.classList.add("idle");
      }
    }

    // Setup voice activity detection
    function setupVoiceActivityDetection(stream) {
      try {
        const audioContext = new (window.AudioContext || window.webkitAudioContext)();
        const analyser = audioContext.createAnalyser();
        const microphone = audioContext.createMediaStreamSource(stream);
        const javascriptNode = audioContext.createScriptProcessor(2048, 1, 1);
        
        analyser.smoothingTimeConstant = 0.8;
        analyser.fftSize = 1024;
        
        microphone.connect(analyser);
        analyser.connect(javascriptNode);
        javascriptNode.connect(audioContext.destination);
        
        // Variables for voice detection
        let silenceStart = null;
        let speechDetected = false;
        const silenceThreshold = 30; // Adjust based on testing
        const silenceDelay = 1500; // 1.5 seconds of silence to stop
        
        javascriptNode.onaudioprocess = function() {
          const array = new Uint8Array(analyser.frequencyBinCount);
          analyser.getByteFrequencyData(array);
          
          // Get average volume level
          let values = 0;
          for (let i = 0; i < array.length; i++) {
            values += array[i];
          }
          const average = values / array.length;
          
          // Detect speech
          if (average > silenceThreshold) {
            if (!speechDetected) {
              speechDetected = true;
              logDebug("üé§ Speech detected");
            }
            silenceStart = null;
          } else if (speechDetected) {
            // Silence after speech
            if (!silenceStart) {
              silenceStart = Date.now();
            } else if (Date.now() - silenceStart > silenceDelay) {
              // Stop recording after silence threshold
              if (mediaRecorder && mediaRecorder.state === "recording") {
                logDebug("üîá Silence detected, stopping recording");
                mediaRecorder.stop();
                
                // Clean up audio processing
                javascriptNode.disconnect();
                analyser.disconnect();
                microphone.disconnect();
                if (audioContext.state !== 'closed') {
                  audioContext.close();
                }
              }
            }
          }
        };
      } catch (e) {
        logDebug("‚ö†Ô∏è Voice activity detection not available: " + e.message);
        // Fall back to timeout-based recording
      }
    }

    async function stopListening() {
      interrupted = true;
      if (mediaRecorder && mediaRecorder.state !== "inactive") {
        mediaRecorder.stop();
      }
      if (stream) {
        stream.getTracks().forEach(track => track.stop());
      }
      if (!audio.paused) {
        audio.pause();
        audio.currentTime = 0;
      }
      removePlayButton();
    }

    async function sendToChat(message) {
      removePlayButton();
      orb.classList.remove("listening", "idle", "speaking");
      orb.classList.add("thinking");
      logDebug("üí¨ Sending to Nag...");

      try {
        const res = await fetch("/chat", {
          method: "POST",
          headers: { 
              "Content-Type": "application/json; charset=utf-8",
              "Accept": "application/json"
          },
          body: JSON.stringify({ message })
        });

        // Get the text first to inspect it if JSON parsing fails
        const rawText = await res.text();
        
        let data;
        try {
            data = JSON.parse(rawText);
        } catch (jsonErr) {
            logDebug("‚ùå Chat response JSON parse failed: " + jsonErr.message);
            logDebug("Raw response: " + rawText.substring(0, 150) + "...");
            orb.classList.remove("thinking");
            orb.classList.add("idle");
            // Continue listening after error
            setTimeout(() => {
              if (!interrupted) startListening();
            }, 1000);
            return;
        }

        logDebug("üß† Nag: " + data.response);
        orb.classList.remove("thinking");

        if (data.audio_url) {
          orb.classList.add("speaking");
          audio.src = data.audio_url;
          audio.load();
          
          // Try to unlock audio for Safari
          if ((isiOS || isSafari) && !audioUnlocked) {
            unlockAudio();
          }
          
          try {
            await audio.play();
            audio.onended = () => {
              orb.classList.remove("speaking");
              orb.classList.add("idle");
              if (!interrupted) startListening();
            };
          } catch (e) {
            logDebug("üîá Audio play failed: " + e.message);
            
            // Add a play button when autoplay fails
            removePlayButton(); // Remove any existing play button first
            
            let playButton = document.createElement("button");
            playButton.innerText = "‚ñ∂Ô∏è Play Response";
            playButton.className = "play-button";
            currentPlayButton = playButton;
            
            // Insert before debug box
            document.body.insertBefore(playButton, debugBox);
            
            playButton.onclick = () => {
              // Unlock audio for future playbacks
              audioUnlocked = true;
              
              audio.play()
                .then(() => {
                  removePlayButton();
                  orb.classList.add("speaking");
                  
                  audio.onended = () => {
                    orb.classList.remove("speaking");
                    orb.classList.add("idle");
                    if (!interrupted) startListening();
                  };
                })
                .catch(err => {
                  logDebug("üîá Manual play failed: " + err.message);
                });
            };
            
            orb.classList.remove("speaking");
            orb.classList.add("idle");
          }
        } else {
          logDebug("‚ö†Ô∏è No audio returned.");
          orb.classList.add("idle");
          // Continue listening after a brief delay
          setTimeout(() => {
            if (!interrupted) startListening();
          }, 1000);
        }
      } catch (e) {
        logDebug("‚ùå Chat error: " + e.message);
        orb.classList.remove("thinking");
        orb.classList.add("idle");
        // Continue listening after error
        setTimeout(() => {
          if (!interrupted) startListening();
        }, 1000);
      }
    }

    // Setup interruption handling
    function setupInterruptionHandling() {
      document.addEventListener('click', function(e) {
        // Don't interrupt if the user is clicking the toggle button or play button
        if (e.target === toggleButton || (currentPlayButton && e.target === currentPlayButton)) {
          return;
        }
        
        // If AI is speaking, interpret click as interruption
        if (!audio.paused && orb.classList.contains("speaking")) {
          logDebug("üîÑ Interrupting AI response...");
          audio.pause();
          audio.currentTime = 0;
          orb.classList.remove("speaking");
          
          // Start listening after interruption
          setTimeout(() => {
            if (!interrupted) startListening();
          }, 500);
        }
      });
    }

    toggleButton.addEventListener("click", async () => {
      // Try to unlock audio on first interaction
      unlockAudio();
      
      if (listening) {
        logDebug("‚èπÔ∏è Stopping conversation...");
        toggleButton.textContent = "Resume Conversation";
        await stopListening();
        orb.classList.remove("listening", "speaking", "thinking");
        orb.classList.add("idle");
      } else {
        logDebug("‚ñ∂Ô∏è Starting conversation...");
        toggleButton.textContent = "Stop Conversation";
        interrupted = false;
        await startListening();
      }
      listening = !listening;
    });

    // Initialize interruption handling
    setupInterruptionHandling();

    // Log initial browser capabilities
    if (window.MediaRecorder) {
      logDebug("‚úÖ MediaRecorder is supported in this browser");
      logDebug(isiOS ? "üì± iOS device detected" : "üíª Desktop browser detected");
      logDebug(isSafari ? "üß≠ Safari browser detected" : "üåê Non-Safari browser detected");
      
      const supportedTypes = [
        "audio/webm", 
        "audio/mp4", 
        "audio/mpeg", 
        "audio/ogg;codecs=opus"
      ];
      for (const type of supportedTypes) {
        logDebug(`${type}: ${MediaRecorder.isTypeSupported(type) ? '‚úÖ' : '‚ùå'}`);
      }
    }
  </script>
</body>
</html>