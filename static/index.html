<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Nag - The Digital Twin</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <style>
    body {
      margin: 0;
      font-family: 'Segoe UI', sans-serif;
      background: linear-gradient(to bottom right, #000000, #1a1a1a);
      color: #fff;
      display: flex;
      flex-direction: column;
      align-items: center;
      justify-content: center;
      height: 100vh;
      overflow: hidden;
    }
    h1 { font-size: 2rem; margin-bottom: 0.2rem; }
    p { margin-top: 0; font-style: italic; opacity: 0.8; }
    #orb {
      width: 120px; height: 120px; border-radius: 50%;
      margin: 2rem auto;
      background: radial-gradient(circle, #00f2ff, #001f3f);
      box-shadow: 0 0 25px rgba(0,255,255,0.6);
      transition: all 0.4s ease-in-out;
      animation: pulseIdle 2s infinite;
    }
    #orb.listening {
      background: radial-gradient(circle, #ff7300, #a62c00);
      animation: pulseListening 1s infinite;
    }
    #orb.speaking {
      background: radial-gradient(circle, #00ff8c, #004d33);
      animation: pulseSpeaking 1s infinite;
    }
    #orb.thinking {
      background: radial-gradient(circle, #ffcc00, #cc7700);
      animation: pulseThinking 1.5s infinite;
    }
    @keyframes pulseIdle {
      0%, 100% { transform: scale(1); box-shadow: 0 0 20px rgba(0,255,255,0.2); }
      50% { transform: scale(1.05); box-shadow: 0 0 30px rgba(0,255,255,0.4); }
    }
    @keyframes pulseListening {
      0%, 100% { transform: scale(1); box-shadow: 0 0 30px rgba(255,115,0,0.3); }
      50% { transform: scale(1.1); box-shadow: 0 0 40px rgba(255,115,0,0.5); }
    }
    @keyframes pulseSpeaking {
      0%, 100% { transform: scale(1); box-shadow: 0 0 30px rgba(0,255,140,0.4); }
      50% { transform: scale(1.1); box-shadow: 0 0 40px rgba(0,255,140,0.6); }
    }
    @keyframes pulseThinking {
      0%, 100% { transform: scale(1); box-shadow: 0 0 30px rgba(255,204,0,0.3); }
      50% { transform: scale(1.08); box-shadow: 0 0 40px rgba(255,204,0,0.5); }
    }
    #debug {
      width: 90%;
      max-height: 250px;
      overflow-y: auto;
      background: #111;
      border: 1px solid #333;
      padding: 10px;
      font-family: monospace;
      font-size: 0.85rem;
      color: #0f0;
      margin-top: 2rem;
    }
    #toggleButton {
      padding: 10px 20px;
      font-size: 1rem;
      margin-top: 1rem;
      border: none;
      background-color: #00f2ff;
      color: #000;
      border-radius: 10px;
      cursor: pointer;
    }
    #toggleButton:disabled {
      background-color: #555;
      cursor: not-allowed;
    }
    .play-button {
      padding: 8px 16px;
      margin: 10px auto;
      display: block;
      background-color: #00ff8c;
      color: #000;
      border: none;
      border-radius: 8px;
      cursor: pointer;
      font-weight: bold;
    }
  </style>
</head>
<body>
  <h1>Welcome to Nag</h1>
  <p>The digital extension of Dinakara's mind ‚Äî therapist, companion, unfiltered mirror.</p>
  <div id="orb" class="idle"></div>
  <audio id="audio" hidden></audio>
  <button id="toggleButton">Start Conversation</button>
  <div id="debug"><strong>Debug Log:</strong></div>

  <script>
    const orb = document.getElementById("orb");
    const audio = document.getElementById("audio");
    const debugBox = document.getElementById("debug");
    const toggleButton = document.getElementById("toggleButton");

    let mediaRecorder;
    let audioChunks = [];
    let stream;
    let listening = false;
    let interrupted = false;
    let currentPlayButton = null;
    let emptyTranscriptionCount = 0;
    let isUploading = false;
    
    // Detect browser environment
    const isiOS = /iPad|iPhone|iPod/.test(navigator.userAgent) || 
        (navigator.platform === 'MacIntel' && navigator.maxTouchPoints > 1);
    const isSafari = /^((?!chrome|android).)*safari/i.test(navigator.userAgent);
    
    // Flag to track if audio has been unlocked
    let audioUnlocked = false;

    // Function to unlock audio
    function unlockAudio() {
      if (audioUnlocked) return Promise.resolve(true);
      
      return new Promise((resolve) => {
        // Create and play a silent audio file
        const silentAudio = new Audio("data:audio/mp3;base64,SUQzBAAAAAAAI1RTU0UAAAAPAAADTGF2ZjU4LjI5LjEwMAAAAAAAAAAAAAAA//tQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAWGluZwAAAA8AAAACAAADQgD///////////////////////////////////////////8AAAA8TEFNRTMuMTAwAQAAAAAAAAAAABSAJAJAQgAAgAAAA0L2YLwAAAAAAAAAAAAAAAAAAAAA//sQZAAP8AAAaQAAAAgAAA0gAAABAAABpAAAACAAADSAAAAETEFNRTMuMTAwVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV//sQZB4P8AAAaQAAAAgAAA0gAAABAAABpAAAACAAADSAAAAEVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVU=");
        
        // Try to play it
        silentAudio.play().then(() => {
          audioUnlocked = true;
          logDebug("üîä Audio unlocked successfully");
          resolve(true);
        }).catch(e => {
          logDebug("‚ö†Ô∏è Could not unlock audio automatically: " + e.message);
          resolve(false);
        });
      });
    }

    // Remove play button if exists
    function removePlayButton() {
      if (currentPlayButton) {
        currentPlayButton.remove();
        currentPlayButton = null;
      }
    }

    // Check if MediaRecorder is supported
    if (!window.MediaRecorder) {
      logDebug("‚ö†Ô∏è Your browser doesn't support MediaRecorder API!");
      toggleButton.disabled = true;
      toggleButton.textContent = "Not supported in this browser";
    }

    function logDebug(msg) {
      const p = document.createElement("p");
      p.textContent = msg;
      debugBox.appendChild(p);
      debugBox.scrollTop = debugBox.scrollHeight;
    }

    async function startListening() {
      if (isUploading) {
        logDebug("‚è≥ Still processing previous request, please wait...");
        return;
      }
      
      try {
        // Clean up any existing play button
        removePlayButton();
        
        // Reset empty transcription counter
        emptyTranscriptionCount = 0;
        
        orb.classList.remove("idle", "speaking", "thinking");
        orb.classList.add("listening");
        logDebug("üéôÔ∏è Listening...");

        stream = await navigator.mediaDevices.getUserMedia({ audio: true });

        // Try different MIME types in order of preference
        let mimeType = "";
        // Prioritize MP4 on iOS/Safari
        const supportedTypes = (isiOS || isSafari) 
          ? ["audio/mp4", "audio/mpeg", "audio/webm", "audio/ogg;codecs=opus", ""]
          : ["audio/webm", "audio/mp4", "audio/mpeg", "audio/ogg;codecs=opus", ""];
        
        for (const type of supportedTypes) {
          if (MediaRecorder.isTypeSupported(type)) {
            mimeType = type;
            logDebug(`Using audio format: ${mimeType || "browser default"}`);
            break;
          }
        }
        
        // Create MediaRecorder with the supported type
        mediaRecorder = new MediaRecorder(stream, mimeType ? { mimeType } : {});
        audioChunks = [];

        mediaRecorder.ondataavailable = e => audioChunks.push(e.data);

        mediaRecorder.onstop = async () => {
          if (interrupted) return;
          orb.classList.remove("listening");
          
          if (audioChunks.length === 0) {
            logDebug("‚ö†Ô∏è No audio recorded. Please try again.");
            orb.classList.add("idle");
            setTimeout(() => {
              if (!interrupted) startListening();
            }, 1000);
            return;
          }
          
          // Use the same MIME type for the blob
          const blob = new Blob(audioChunks, mimeType ? { type: mimeType } : {});
          const formData = new FormData();
          
          // Determine appropriate file extension based on MIME type
          let fileExt = "audio";
          if (mimeType.includes("webm")) fileExt = "webm";
          else if (mimeType.includes("mp4") || mimeType.includes("mpeg")) fileExt = "mp3";
          else if (mimeType.includes("ogg")) fileExt = "ogg";
          
          formData.append("file", blob, `input.${fileExt}`);

          try {
            // Set uploading flag
            isUploading = true;
            logDebug("üì§ Uploading voice...");
            
            const res = await fetch("/transcribe", { 
              method: "POST", 
              body: formData 
            });
            
            // Clear uploading flag
            isUploading = false;

            let data;
            try {
              const rawText = await res.text();
              try {
                data = JSON.parse(rawText);
              } catch (jsonErr) {
                logDebug("‚ùå JSON parse failed: " + jsonErr.message);
                logDebug("Raw response: " + rawText.substring(0, 100) + "...");
                orb.classList.add("idle");
                // Continue listening after a brief delay
                setTimeout(() => {
                  if (!interrupted) startListening();
                }, 1000);
                return;
              }
            } catch (textErr) {
              logDebug("‚ùå Failed to read response: " + textErr.message);
              orb.classList.add("idle");
              setTimeout(() => {
                if (!interrupted) startListening();
              }, 1000);
              return;
            }

            const message = (data.transcription || "").trim();
            logDebug("üìù Transcribed: " + (message || "No speech detected"));

            if (!message || message === "undefined") {
              logDebug("‚ö†Ô∏è Empty message. Continuing to listen...");
              emptyTranscriptionCount++;
              
              // If we get too many empty transcriptions in a row, prompt the user
              if (emptyTranscriptionCount >= 3) {
                emptyTranscriptionCount = 0; // Reset counter
                await sendToChat("I didn't hear anything. How can I help you today?");
              } else {
                orb.classList.add("idle");
                // Continue listening after a brief delay
                setTimeout(() => {
                  if (!interrupted) startListening();
                }, 1000);
              }
              return;
            }

            // Reset counter on successful transcription
            emptyTranscriptionCount = 0;
            await sendToChat(message);
          } catch (e) {
            // Clear uploading flag
            isUploading = false;
            logDebug("‚ùå Transcription error: " + e.message);
            orb.classList.add("idle");
            // Continue listening after a brief delay
            setTimeout(() => {
              if (!interrupted) startListening();
            }, 1000);
          }
        };

        // Start recording with fixed duration (more reliable than voice detection)
        mediaRecorder.start();
        setTimeout(() => {
          if (mediaRecorder && mediaRecorder.state === "recording") {
            mediaRecorder.stop();
          }
        }, 5000); // 5 seconds recording - adjust as needed
      } catch (e) {
        logDebug("üö´ Mic access failed: " + e.message);
        orb.classList.remove("listening");
        orb.classList.add("idle");
      }
    }

    async function stopListening() {
      interrupted = true;
      if (mediaRecorder && mediaRecorder.state !== "inactive") {
        mediaRecorder.stop();
      }
      if (stream) {
        stream.getTracks().forEach(track => track.stop());
      }
      if (!audio.paused) {
        audio.pause();
        audio.currentTime = 0;
      }
      removePlayButton();
    }

    async function sendToChat(message) {
      if (isUploading) {
        logDebug("‚è≥ Still processing previous request, please wait...");
        return;
      }
      
      removePlayButton();
      orb.classList.remove("listening", "idle", "speaking");
      orb.classList.add("thinking");
      logDebug("üí¨ Sending to Nag...");

      try {
        isUploading = true; // Set uploading flag
        const res = await fetch("/chat", {
          method: "POST",
          headers: { 
              "Content-Type": "application/json; charset=utf-8",
              "Accept": "application/json"
          },
          body: JSON.stringify({ message })
        });
        isUploading = false; // Clear uploading flag

        if (!res.ok) {
          throw new Error(`Server error: ${res.status} ${res.statusText}`);
        }

        // Get the text first to inspect it if JSON parsing fails
        const rawText = await res.text();
        
        let data;
        try {
            data = JSON.parse(rawText);
        } catch (jsonErr) {
            logDebug("‚ùå Chat response JSON parse failed: " + jsonErr.message);
            logDebug("Raw response: " + rawText.substring(0, 150) + "...");
            orb.classList.remove("thinking");
            orb.classList.add("idle");
            // Continue listening after error
            setTimeout(() => {
              if (!interrupted) startListening();
            }, 1000);
            return;
        }

        logDebug("üß† Nag: " + data.response);
        orb.classList.remove("thinking");

        if (data.audio_url) {
          orb.classList.add("speaking");
          
          // Preload audio for faster response
          try {
            // Ensure audio is ready
            await new Promise((resolve) => {
              const onready = () => {
                audio.oncanplaythrough = null;
                audio.onerror = null;
                resolve();
              };
              audio.oncanplaythrough = onready;
              audio.onerror = onready; // Still resolve on error to avoid hanging
              audio.src = data.audio_url;
              audio.load();
            });
          } catch (e) {
            logDebug("‚ö†Ô∏è Audio preload warning: " + e.message);
          }
          
          // Try to unlock audio for Safari
          await unlockAudio();
          
          try {
            let playResult = audio.play();
            
            // Set up onended handler
            audio.onended = () => {
              orb.classList.remove("speaking");
              orb.classList.add("idle");
              if (!interrupted) startListening();
            };
            
            // Handle the play promise
            if (playResult !== undefined) {
              playResult.catch(e => {
                logDebug("üîá Audio play failed: " + e.message);
                showPlayButton(data.audio_url);
              });
            }
          } catch (e) {
            logDebug("üîá Audio play exception: " + e.message);
            showPlayButton(data.audio_url);
          }
        } else {
          logDebug("‚ö†Ô∏è No audio returned.");
          orb.classList.add("idle");
          // Continue listening after a brief delay
          setTimeout(() => {
            if (!interrupted) startListening();
          }, 1000);
        }
      } catch (e) {
        isUploading = false; // Clear uploading flag on error
        logDebug("‚ùå Chat error: " + e.message);
        orb.classList.remove("thinking");
        orb.classList.add("idle");
        // Continue listening after error
        setTimeout(() => {
          if (!interrupted) startListening();
        }, 1000);
      }
    }
    
    // Function to show play button
    function showPlayButton(audioUrl) {
      orb.classList.remove("speaking", "thinking");
      orb.classList.add("idle");
      
      removePlayButton();
      
      let playButton = document.createElement("button");
      playButton.innerText = "‚ñ∂Ô∏è Play Response";
      playButton.className = "play-button";
      currentPlayButton = playButton;
      
      // Insert before debug box
      document.body.insertBefore(playButton, debugBox);
      setTimeout(() => playButton.focus(), 100);
      
      playButton.onclick = () => {
        // Unlock audio for future playbacks
        audioUnlocked = true;
        
        orb.classList.remove("idle");
        orb.classList.add("speaking");
        
        audio.src = audioUrl;
        audio.load();
        
        audio.play()
          .then(() => {
            removePlayButton();
          })
          .catch(err => {
            logDebug("üîá Manual play failed: " + err.message);
            orb.classList.remove("speaking");
            orb.classList.add("idle");
          });
        
        audio.onended = () => {
          orb.classList.remove("speaking");
          orb.classList.add("idle");
          if (!interrupted) startListening();
        };
      };
    }

    // Setup interruption handling
    function setupInterruptionHandling() {
      document.addEventListener('click', function(e) {
        // Don't interrupt if the user is clicking the toggle button or play button
        if (e.target === toggleButton || 
            (currentPlayButton && (e.target === currentPlayButton || currentPlayButton.contains(e.target)))) {
          return;
        }
        
        // If AI is speaking, interpret click as interruption
        if (orb.classList.contains("speaking")) {
          logDebug("üîÑ Interrupting AI response...");
          audio.pause();
          audio.currentTime = 0;
          orb.classList.remove("speaking");
          orb.classList.add("idle");
          
          // Start listening after interruption
          setTimeout(() => {
            if (!interrupted) startListening();
          }, 500);
        }
      });
    }

    toggleButton.addEventListener("click", async () => {
      // Try to unlock audio on first interaction
      await unlockAudio();
      
      if (listening) {
        logDebug("‚èπÔ∏è Stopping conversation...");
        toggleButton.textContent = "Resume Conversation";
        await stopListening();
        orb.classList.remove("listening", "speaking", "thinking");
        orb.classList.add("idle");
      } else {
        logDebug("‚ñ∂Ô∏è Starting conversation...");
        toggleButton.textContent = "Stop Conversation";
        interrupted = false;
        await startListening();
      }
      listening = !listening;
    });

    // Initialize interruption handling
    setupInterruptionHandling();

    // Log initial browser capabilities
    if (window.MediaRecorder) {
      logDebug("‚úÖ MediaRecorder is supported in this browser");
      logDebug(isiOS ? "üì± iOS device detected" : "üíª Desktop browser detected");
      logDebug(isSafari ? "üß≠ Safari browser detected" : "üåê Non-Safari browser detected");
      
      const supportedTypes = [
        "audio/webm", 
        "audio/mp4", 
        "audio/mpeg", 
        "audio/ogg;codecs=opus"
      ];
      for (const type of supportedTypes) {
        logDebug(`${type}: ${MediaRecorder.isTypeSupported(type) ? '‚úÖ' : '‚ùå'}`);
      }
    }
  </script>
</body>
</html>