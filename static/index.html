<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Nag – The Digital Extension</title>
    <style>
        body {
            margin: 0;
            background: linear-gradient(to bottom right, #1a1a2e, #16213e);
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            color: white;
            text-align: center;
            padding-top: 40px;
        }

        h1 {
            font-size: 2.5rem;
        }

        p {
            margin-top: 0;
            font-size: 1.2rem;
        }

        #circle {
            width: 200px;
            height: 200px;
            margin: 30px auto;
            border-radius: 50%;
            background: radial-gradient(circle at center, #6c5ce7, #341f97);
            animation: pulse 2s infinite ease-in-out;
        }

        @keyframes pulse {
            0% { transform: scale(1); opacity: 0.8; }
            50% { transform: scale(1.1); opacity: 1; }
            100% { transform: scale(1); opacity: 0.8; }
        }

        button {
            padding: 12px 24px;
            font-size: 16px;
            margin-top: 20px;
            background-color: #00b894;
            color: white;
            border: none;
            border-radius: 8px;
            cursor: pointer;
        }

        audio {
            margin-top: 30px;
        }
    </style>
</head>
<body>
    <h1>Nag</h1>
    <p>The digital extension of Dinakara’s mind — therapist, companion, unfiltered mirror.</p>

    <div id="circle"></div>

    <button onclick="startRecording()">Talk to Nag</button>
    <p id="statusText">Click the button and start speaking</p>
    <audio id="nagAudio" controls></audio>

    <script>
        let mediaRecorder, audioChunks = [];

        async function startRecording() {
            const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
            mediaRecorder = new MediaRecorder(stream);
            audioChunks = [];

            mediaRecorder.ondataavailable = event => {
                audioChunks.push(event.data);
            };

            mediaRecorder.onstop = async () => {
                const audioBlob = new Blob(audioChunks, { type: 'audio/wav' });
                const text = await transcribeAudio(audioBlob);
                if (text) {
                    document.getElementById("statusText").innerText = "Nag is thinking...";
                    const response = await fetch('/chat', {
                        method: 'POST',
                        headers: { 'Content-Type': 'application/json' },
                        body: JSON.stringify({ message: text })
                    });

                    const data = await response.json();
                    if (data.audio_url) {
                        document.getElementById("nagAudio").src = data.audio_url;
                        document.getElementById("statusText").innerText = "Nag replied!";
                        document.getElementById("nagAudio").play();
                    }
                }
            };

            mediaRecorder.start();
            setTimeout(() => mediaRecorder.stop(), 5000); // Record for 5 seconds
            document.getElementById("statusText").innerText = "Listening...";
        }

        async function transcribeAudio(audioBlob) {
            // Dummy placeholder: Replace with Whisper API or your transcription logic
            alert("⚠️ Transcription not implemented. Please add Whisper API call.");
            return "This is a test question to Nag";
        }
    </script>
</body>
</html>
